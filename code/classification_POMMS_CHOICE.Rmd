---
title: "R Notebook"
output: html_notebook
---

# Libraries

```{r, include = FALSE}
library(tidyverse)
library(microbiome)
library(phyloseq)
library(caret)
library(randomForest)
library(MButils)
```

# Load data

I am using the AHA dataset for writing my code because it is the simplest dataset I can work with.
Simplest = significant number of baseline samples without issue of repeated measures, balanced distribution of classes, likely difference to be detected present.


```{r}
p_ps = readRDS("/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/POMMS/R24_POMMS/data/processed/20221213_pomms_cleanmetadata_plant.rds")

p_ps

sample_data(p_ps)
```

```{r}
c_ps = readRDS("/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/CHOICE/CHOICE_Trnl/CHOICE_20220912/ps_objects/choice_complete_20221205_clean.rds")

c_ps
```

# Preprocess

## Remove problematic samples

Taking out samples with less than or equal to 0 reads and keeping only entry timepoint reduces samples from 384 down to 240.

```{r}
p_ps = p_ps %>%
  subset_samples( reads > 0) %>%
  subset_samples(timepoint == "Entry")

p_ps
```

Class distribution in this dataset:
Case = 193
Control = 47

```{r}
sample_data(p_ps) %>%
  as.data.frame() %>%
  as_tibble() %>%
  group_by(group) %>%
  summarise(count = n())
```
## Merge phyloseq

```{r}
combined = merge_phyloseq(c_ps, p_ps)

combined
```

### Aligning seq table

```{r}
seqtab = combined@otu_table

seqtab_merged = dada2::collapseNoMismatch(seqtab)

combined = phyloseq(otu_table(seqtab_merged, taxa_are_rows=FALSE),
               sample_data(combined@sam_data), 
               tax_table(combined@tax_table))

combined

#sample_data(combined)
```


## Data transform

```{r}
otu_clr = abundances(combined, "clr") %>% 
  t()

combined = phyloseq(otu_table(otu_clr, taxa_are_rows = FALSE),
                   sample_data(combined@sam_data),
                   tax_table(combined@tax_table))

rm(otu_clr)
```

## Feature table

### Extract data

Order of asv colnames in OTU table and tax table is the same.

```{r}
features = combined@otu_table %>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample")

feature_labels = c("sample")

features
```

### Zero variance features

No features have zero variance
```{r}
nzv = features %>%
  select(!sample) %>%
  nearZeroVar(saveMetrics = TRUE)
```

```{r}
nzv %>%
  filter(nzv == TRUE)
```


```{r}
rm(nzv)
```

### Correlated predictors

```{r}
feature_cor = features %>%
 select(!sample) %>%
 cor() 

summary(feature_cor[upper.tri(feature_cor)])
```

```{r}
high_cor = findCorrelation(feature_cor, cutoff = .50) #using a more permissive cutoff because a lot of columns still left in the input table

filtered_features = features[,-high_cor]
```

```{r}
rm(high_cor)
rm(feature_cor)
```

### Linear dependencies

```{r}
feature_ld = filtered_features %>%
  select(!sample) %>%
  findLinearCombos()

filtered_features = filtered_features[, - feature_ld$remove]
```

```{r}
rm(feature_ld)
```

## Input table
```{r}
sample_labels = combined@sam_data%>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample") %>%
  select(sample, group) %>%
  left_join(filtered_features)

input = sample_labels %>%
  filter(str_detect(sample, "v")) %>% # change here
  mutate(group_shuffle = sample(group))

external_test = sample_labels %>%
  filter(!str_detect(sample, "v")) %>% # change here
  select(-c(group))
```

# Modeling

## Baseline performance

```{r}
label_list = c("group", "group_shuffle")
results = loop_label(iterations =20, input, label_list, percent = 0.8, upsampling = TRUE)

auc_df = results[[2]]

importance_df = results[[1]]
```

```{r}
auc_df %>%
  pivot_longer(cols = c("group", "group_shuffle"), names_to = "feature", values_to = "AUC") %>%
  ggplot()+
  geom_boxplot(aes(x=feature, y = AUC))+
  theme_classic()

t.test(auc_df$group, auc_df$group_shuffle)

```

```{r}
rm(auc_df)

rm(importance_df)
```

## Loop

```{r}
label = "group"

importance_df = data.frame()

cummulative_external = data.frame()

auc_list = c()

current_data = input %>%
      select(-label_list[label_list != label]) %>%
      select(-sample)

for(i in 1:20) {
  # Data splitting
    train_ind = caret::createDataPartition((current_data %>%
                                              pull(label)), #split based on label column
                                     p = 0.8,
                                     list = FALSE,
                                     times = 1)
    train = current_data[train_ind,]
    test = current_data[-train_ind,]
    
    # Upsampling
    train[[label]] = as.factor(train[[label]])
    train = train %>%
        as.data.frame()
      
    formula_smote = label %>%
      paste0("~.") %>%
      as.formula()
      
    train = DMwR::SMOTE(formula_smote, data  = train)
    
    # Training model
    
    formula = label %>%
      paste0("~.") %>%
      as.formula()
    
    fit_control = caret::trainControl(method = "CV", # 5-fold cross validation
                                 number = 5,
                                 summaryFunction = prSummary,
                                 classProbs = T)
    
    model = caret::train(formula, #using formula specified 
              data = train,
                  method = "rf",
                  trControl = fit_control,
                  verbose = FALSE, 
                  metric = "AUC")
    
    # Model predictions
    
    predictions = predict(model, newdata = test, type= "prob")
    
    predictions$obs = test %>%
      pull(label) %>%
      as.factor() 
    
    # Evaluate model
    
    eval = MLeval::evalm(predictions, showplots = FALSE, silent = TRUE)
    
    auc = eval$stdres$`Group1`[13,1]
    
    auc_list = append(auc_list, auc)
    
    #Extract feature importance

    feature_importance = caret::varImp(model$finalModel) %>% # Ben calls this importances instead of final model but the rankings are exactly the same, only number value scale is different.
      arrange(desc(Overall)) %>%
      as.matrix() %>%
      t() %>%
      as.data.frame()
    
    feature_importance$auc = auc
    
    feature_importance$iteration = i
    
    feature_importance$label = label
    
    importance_df = rbind(importance_df, feature_importance)
    
    #Predicting on external dataset
    
    external_predictions =  predict(model, newdata = external_test, type= "prob")
    
    external_predictions$sample = external_test$sample
    
    external_predictions$auc = auc
    
    external_predictions$pred = predict(model, newdata = external_test)%>%
      as.factor ()
    
    external_predictions$iteration = i
    
    cummulative_external = rbind(cummulative_external, external_predictions)
    
}

auc_list

importance_df

cummulative_external  
  
```

# Plotting

## AUC plot and t-test
```{r}
data.frame(auc = auc_list)%>%
  ggplot()+
  geom_boxplot(aes(y = auc))+
  theme_classic()
```

## Save external predictions

### predictions
```{r}
samdf = c_ps@sam_data%>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample") %>%
  select(sample, id, treatment, true_week, timepoint) 

save = cummulative_external %>%
  #filter(auc == max) %>%
  left_join(samdf) %>%
  arrange(iteration, treatment, id, true_week)
```

```{r}
write_csv(save, "/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/Machine_learning/LAD_lab_ML/code/pomms_to_choice_predictions.csv")
```


### variable importances

```{r}
write_csv(importance_df, "/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/Machine_learning/LAD_lab_ML/code/pomms_to_choice_feature_importance.csv")
```
