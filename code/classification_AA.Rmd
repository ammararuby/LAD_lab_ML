---
title: "R Notebook"
output: html_notebook
---

# Libraries

```{r, echo = FALSE}
library(tidyverse)
library(microbiome)
library(phyloseq)
library(caret)
library(randomForest)
library(MButils)
library(pROC)
```

# Load data

I am using the AHA dataset for writing my code because it is the simplest dataset I can work with.
Simplest = significant number of baseline samples without issue of repeated measures, balanced distribution of classes, likely difference to be detected present.


```{r}
a_ps = readRDS("/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/AHA/220714_MN00462_0226_A000H3WKM5/20221110_results/ps_objects/AHA_trnl_with_metadata_20221205.rds")

a_ps
```

# Preprocess

## Remove problematic samples

Taking out samples that are not listed to have completed the study and samples with more than 0 reads reduces samples from 95 down to 77.

Taking only the baseline timepoint brings further down to 44 samples

```{r}
a_ps = a_ps %>%
  subset_samples(completion == "Completed" & reads >0) %>%
  subset_samples(timepoint == "Baseline")

a_ps
```

Class distribution in this dataset:
Case = 20
Control = 24

```{r}
sample_data(a_ps) %>%
  as.data.frame() %>%
  as_tibble() %>%
  group_by(group) %>%
  summarise(count = n())
```
## Remove unassigned taxa

Debating whether this should be done for machine learning. Because we are simply throwing out because we don't know what there are and even if they account for a small percentage of reads, they might still have important information.

I think I will refrain from removing unless I have a good reason to.

Removing completely unassigned taxa reduces taxa from 235 to 82.

```{r}
a_ps = a_ps %>%
  subset_taxa(is.na(superkingdom) == FALSE)

a_ps
```

## Data transform

```{r}
otu_clr = abundances(a_ps, "clr") %>% # rclr transform could not be performed, get Nans
  t()

a_ps = phyloseq(otu_table(otu_clr, taxa_are_rows = FALSE),
                   sample_data(sample_data(a_ps)),
                   tax_table(tax_table(a_ps)))

rm(otu_clr)
```

## Feature table

### Extract data
```{r}
features = a_ps@otu_table %>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample")

feature_labels = c("sample")

names = tax_table(a_ps) %>% 
     data.frame() %>% 
     lowest_level() %>%
  pull(name)

feature_labels = append(feature_labels, names)

feature_labels = make.unique(feature_labels) %>%
  make.names()

colnames(features) = feature_labels

features
```
### Zero variance features

No features have zero variance
```{r}
nzv = features %>%
  select(!sample) %>%
  nearZeroVar(saveMetrics = TRUE)
```

```{r}
rm(nzv)
```

### Correlated predictors

```{r}
feature_cor = features %>%
 select(!sample) %>%
 cor() 

summary(feature_cor[upper.tri(feature_cor)])
```

```{r}
high_cor = findCorrelation(feature_cor, cutoff = .75)

filtered_features = features[,-high_cor]
```

```{r}
rm(high_cor)
rm(feature_cor)
```

### Linear dependencies

```{r}
feature_ld = filtered_features %>%
  select(!sample) %>%
  findLinearCombos()

filtered_features = filtered_features[, - feature_ld$remove]
```

## Input table
```{r}
sample_labels = a_ps@sam_data%>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample") %>%
  select(sample, group) %>%
  mutate(group_shuffle = sample(group))

input = sample_labels %>%
  left_join(filtered_features)
```

# Data splitting

```{r}
set.seed(3456)
train_ind = createDataPartition(input$group, p = .7, 
                                  list = FALSE, 
                                  times = 1)

train = input[train_ind,]
test = input[-train_ind,]

```
# Model 

## Training

```{r}
fit_control = trainControl(method = "CV",
                                 number = 5,
                                 summaryFunction = prSummary,
                                 classProbs = T)

model = train(group ~ ., 
              data = select(train, -c(sample, group_shuffle)),
                  method = "rf",
                  trControl = fit_control,
                  verbose = FALSE, 
                  metric = "AUC")

model
```

## Predictions
```{r}
predictions = predict(model, newdata = select(test, -c(group, group_shuffle, sample)), type= "prob")

predictions$obs = test$group

predictions = predictions %>%
  mutate(pred = ifelse(Control > Case, "Control", "Case"),
         pred_num = ifelse(pred == "Control", 1, 0),
         obs_num = ifelse(obs == "Control", 1, 0),
         pred = as.factor(pred),
         obs = as.factor(obs))

predictions
```

## Metrics

### Confusion matrix

```{r}
cf = confusionMatrix(data = as.factor(predictions$pred), reference =as.factor(predictions$obs))

cf
```

### ROC/ Precision-recall

```{r}
library(MLmetrics)
```


Caret does not seem to have built-in ROC curve or precrision recall curve plotting features.

```{r}
# Caret functions
twoClassSummary(predictions, lev = levels(predictions$obs))

prSummary(predictions, lev = levels(predictions$obs))
```


```{r}
# Non-caret functions
result_roc = roc(response = predictions$obs_num, predictor = predictions$Control, auc=TRUE)

plot(result_roc, print.thres="best", print.thres.best.method="closest.topleft")

result_coords = coords(result_roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
```

### Variable importances

I need to make this more robust. Probably better to have as input column names stay as ASVs and then join back to common names at the end. Will ensure I know what the actual taxa are.
```{r}
asv = varImp(model$finalModel) %>%
  arrange(desc(Overall)) %>%
  rownames_to_column("asv")

asv
```







