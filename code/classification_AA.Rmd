---
title: "R Notebook"
output: html_notebook
---

# Libraries

```{r, include = FALSE}
library(tidyverse)
library(microbiome)
library(phyloseq)
library(caret)
library(randomForest)
library(MButils)
library(MLeval)
library(pROC)
```

# Load data

I am using the AHA dataset for writing my code because it is the simplest dataset I can work with.
Simplest = significant number of baseline samples without issue of repeated measures, balanced distribution of classes, likely difference to be detected present.


```{r}
a_ps = readRDS("/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/AHA/220714_MN00462_0226_A000H3WKM5/20221110_results/ps_objects/AHA_trnl_with_metadata_20221205.rds")

a_ps
```

# Preprocess

## Remove problematic samples

Taking out samples that are not listed to have completed the study and samples with more than 0 reads reduces samples from 95 down to 77.

Taking only the baseline timepoint brings further down to 44 samples

```{r}
a_ps = a_ps %>%
  subset_samples(completion == "Completed" & reads >0) %>%
  subset_samples(timepoint == "Baseline")

a_ps
```

Class distribution in this dataset:
Case = 20
Control = 24

```{r}
sample_data(a_ps) %>%
  as.data.frame() %>%
  as_tibble() %>%
  group_by(group) %>%
  summarise(count = n())
```
## Remove unassigned taxa

Debating whether this should be done for machine learning. Because we are simply throwing out because we don't know what there are and even if they account for a small percentage of reads, they might still have important information.

I think I will refrain from removing unless I have a good reason to.

Removing completely unassigned taxa reduces taxa from 235 to 82.

```{r}
a_ps = a_ps %>%
  subset_taxa(is.na(superkingdom) == FALSE)

a_ps
```

## Data transform

```{r}
otu_clr = abundances(a_ps, "clr") %>% # rclr transform could not be performed, get Nans
  t()

a_ps = phyloseq(otu_table(otu_clr, taxa_are_rows = FALSE),
                   sample_data(sample_data(a_ps)),
                   tax_table(tax_table(a_ps)))

rm(otu_clr)
```

## Feature table

### Extract data
```{r}
features = a_ps@otu_table %>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample")

feature_labels = c("sample")

names = tax_table(a_ps) %>% 
     data.frame() %>% 
     lowest_level() %>%
  pull(name)

feature_labels = append(feature_labels, names)

feature_labels = make.unique(feature_labels) %>%
  make.names()

colnames(features) = feature_labels

features
```
### Zero variance features

No features have zero variance
```{r}
nzv = features %>%
  select(!sample) %>%
  nearZeroVar(saveMetrics = TRUE)
```

```{r}
rm(nzv)
```

### Correlated predictors

```{r}
feature_cor = features %>%
 select(!sample) %>%
 cor() 

summary(feature_cor[upper.tri(feature_cor)])
```

```{r}
high_cor = findCorrelation(feature_cor, cutoff = .75)

filtered_features = features[,-high_cor]
```

```{r}
rm(high_cor)
rm(feature_cor)
```

### Linear dependencies

```{r}
feature_ld = filtered_features %>%
  select(!sample) %>%
  findLinearCombos()

filtered_features = filtered_features[, - feature_ld$remove]
```

## Input table
```{r}
sample_labels = a_ps@sam_data%>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample") %>%
  select(sample, group) %>%
  mutate(group_shuffle = sample(group))

input = sample_labels %>%
  left_join(filtered_features)
```

# Modeling

Load functions from classification_function_AA file

```{r}
label_list = c("group", "group_shuffle")
results = loop_label(iterations = 10, input, percent = 0.7)
```

# Plotting

Create visualization functions or code.
```{r}

```

