---
title: "R Notebook"
output: html_notebook
---

# Libraries

```{r, include = FALSE}
library(tidyverse)
library(microbiome)
library(phyloseq)
library(caret)
library(randomForest)
library(MButils)
library(MLeval)
library(pROC)
```

# Load data

I am using the AHA dataset for writing my code because it is the simplest dataset I can work with.
Simplest = significant number of baseline samples without issue of repeated measures, balanced distribution of classes, likely difference to be detected present.


```{r}
a_ps = readRDS("/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/AHA/220714_MN00462_0226_A000H3WKM5/20221110_results/ps_objects/AHA_trnl_with_metadata_20221205.rds")

a_ps
```

# Preprocess

## Remove problematic samples

Taking out samples that are not listed to have completed the study and samples with more than 0 reads reduces samples from 95 down to 77.

Taking only the baseline timepoint brings further down to 44 samples

```{r}
a_ps = a_ps %>%
  subset_samples(completion == "Completed" & reads >0) %>%
  subset_samples(timepoint == "Baseline")

a_ps
```

Class distribution in this dataset:
Case = 20
Control = 24

```{r}
sample_data(a_ps) %>%
  as.data.frame() %>%
  as_tibble() %>%
  group_by(group) %>%
  summarise(count = n())
```
## Remove unassigned taxa

Debating whether this should be done for machine learning. Because we are simply throwing out because we don't know what there are and even if they account for a small percentage of reads, they might still have important information.

I think I will refrain from removing unless I have a good reason to.

Removing completely unassigned taxa reduces taxa from 235 to 82.

```{r}
a_ps = a_ps %>%
  subset_taxa(is.na(superkingdom) == FALSE)

a_ps
```

## Data transform

```{r}
otu_clr = abundances(a_ps, "clr") %>% # rclr transform could not be performed, get Nans
  t()

a_ps = phyloseq(otu_table(otu_clr, taxa_are_rows = FALSE),
                   sample_data(sample_data(a_ps)),
                   tax_table(tax_table(a_ps)))

rm(otu_clr)
```

## Feature table

### Extract data
```{r}
features = a_ps@otu_table %>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample")

feature_labels = c("sample")

names = tax_table(a_ps) %>% 
     data.frame() %>% 
     lowest_level() %>%
  pull(name)

feature_labels = append(feature_labels, names)

feature_labels = make.unique(feature_labels) %>%
  make.names()

colnames(features) = feature_labels

features
```
### Zero variance features

No features have zero variance
```{r}
nzv = features %>%
  select(!sample) %>%
  nearZeroVar(saveMetrics = TRUE)
```

```{r}
rm(nzv)
```

### Correlated predictors

```{r}
feature_cor = features %>%
 select(!sample) %>%
 cor() 

summary(feature_cor[upper.tri(feature_cor)])
```

```{r}
high_cor = findCorrelation(feature_cor, cutoff = .75)

filtered_features = features[,-high_cor]
```

```{r}
rm(high_cor)
rm(feature_cor)
```

### Linear dependencies

```{r}
feature_ld = filtered_features %>%
  select(!sample) %>%
  findLinearCombos()

filtered_features = filtered_features[, - feature_ld$remove]
```

## Input table
```{r}
sample_labels = a_ps@sam_data%>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample") %>%
  select(sample, group) %>%
  mutate(group_shuffle = sample(group))

input = sample_labels %>%
  left_join(filtered_features)
```

# Data splitting

```{r}
set.seed(3456)
train_ind = createDataPartition(input$group, p = .7, 
                                  list = FALSE, 
                                  times = 1)

train = input[train_ind,]
test = input[-train_ind,]

```

# Model 

## Training

```{r}
fit_control = trainControl(method = "CV",
                                 number = 5,
                                 summaryFunction = prSummary,
                                 classProbs = T)

model = train(group ~ ., 
              data = select(train, -c(sample, group_shuffle)),
                  method = "rf",
                  trControl = fit_control,
                  verbose = FALSE, 
                  metric = "AUC")

model
```

## Predictions

```{r}
predictions = predict(model, newdata = select(test, -c(group, group_shuffle, sample)), type= "prob")

labels = predict(model, newdata = select(test, -c(group, group_shuffle, sample))) %>%
  as.factor

predictions$obs = test$group %>%
  as.factor()

predictions
```

## Metrics

### Confusion matrix

```{r}
cf = confusionMatrix(data = labels, reference =as.factor(predictions$obs), mode = "prec_recall")

cf
```

### ROC/ Precision-recall

```{r}
library(MLmetrics)
library(MLeval)
```


Caret does not seem to have built-in ROC curve or precision recall curve plotting features.

```{r}
evalm(predictions)
```

### Variable importances

I need to make this more robust. Probably better to have as input column names stay as ASVs and then join back to common names at the end. Will ensure I know what the actual taxa are.

```{r}
asv = varImp(model$finalModel) %>%
  arrange(desc(Overall)) %>%
  rownames_to_column("asv")

asv
```

# Function version

```{r}
input = input %>%
  select(-c(sample, group_shuffle))
```
## testing
```{r}
# testing function

label = "group"
label_list = c("group", "group_shuffle")
input_cf = 0
auc_list = c()
importance_df = data.frame()
percent = 0.7
iterations = 2

results_2 = loop_model(iterations = 2, current_data, label, percent, input_cf, auc_list, importance_df)

results

results[[1]]

results_2[[2]]

# The function is working!
```

```{r}
results = loop_label(iterations = 2, input, label_list = c("group", "group_shuffle"), percent = 0.7)

input
```

## loop_label
```{r}
loop_label = function(iterations, input, label_list, percent){
  
  #Initialize results
  
  auc_df = data.frame(row = 1:iterations)
  #cf_list = list()
  results_compile = list()
  
  #Label loop
  
  for(label in label_list){
    
    # Remove labels not being tested in this iteration
    
    current_data = input %>%
      select(-label_list[label_list != label]) %>%
      select(-sample)
  
    # Initialize results
    
    input_cf = 0
    auc_list = c()
    importance_df = data.frame()
  
    #Model loop
    
    results = loop_model(iterations, current_data, label, percent, input_cf, auc_list, importance_df)
    
    # Print cummulative confusion matrix
    
    # print(label) # eventually want to make this a result that is stored instead of printing like this.
    # print(results[[1]])
    
    # Store AUC values in cummulative dataframe
    
    auc_df = auc_df %>%
      cbind(results[[2]])
    
    # After this Ben has functions to plot importances- I will need to code.
    
    # After this Ben plots AUCs, I will plot outside the function
    
    # Returning results from all labels
    
    results_compile = results_compile %>%
      append(results)
  }
  
  final = list(results_compile, auc_df)
  
  return(final)
}
```

##loop_model
```{r}
# remove extraneous columns from input before putting into this function

loop_model = function(iterations, current_data, label, percent = 0.7, input_cf, auc_list, importance_df){
  
  for (i in 1:iterations) {
    
    # Data splitting
    train_ind = caret::createDataPartition((current_data %>%
      pull(label)), #split based on label column
                                     p = percent,
                                     list = FALSE,
                                     times = 1)
    train = current_data[train_ind,]
    test = current_data[-train_ind,]
    
    # Training model
    formula = label %>%
      paste0("~.") %>%
      as.formula()
    
    fit_control = caret::trainControl(method = "CV", # 5-fold cross validation
                                 number = 5,
                                 summaryFunction = prSummary,
                                 classProbs = T)
    
    model = caret::train(formula, #using formula specified 
              data = train,
                  method = "rf",
                  trControl = fit_control,
                  verbose = FALSE, 
                  metric = "AUC")
    
    # Model predictions
    predictions = predict(model, newdata = test, type= "prob")
    
    pred_labels = predict(model, newdata = test)%>%
      as.factor ()
    
    predictions$obs = test %>%
      pull(label) %>%
      as.factor() 
    
    # Confusion matrix
    cf = caret::confusionMatrix(data = pred_labels, reference =as.factor(predictions$obs), mode = "prec_recall")
    
    input_cf = input_cf + cf$table
    
    # Evaluate model
    
    eval = MLeval::evalm(predictions, showplots = FALSE, silent = TRUE)
    
    auc_list = append(auc_list, eval$stdres$`Group1`[13,1])
    
    #Extract feature importance
    feature_importance = caret::varImp(model$finalModel) %>% # Ben calls this importances instead of final model but the rankings are exactly the same, only number value scale is different.
      arrange(desc(Overall)) %>%
      as.matrix() %>%
      t()
    
    importance_df = rbind(importance_df, feature_importance)
    
  }
  
  #Naming all results with appropriate label
  
  input_cf = list(label, input_cf)
  
  auc_list = auc_list %>% as.data.frame()
  colnames(auc_list) = label
  
  importance_df = importance_df %>%
    mutate(variable = label)
  
  #Join all the result elements together in a list: cf, auc vector, importance df
  results_list = list(input_cf, auc_list, importance_df)
  
  return(results_list) 
}
```


# Backup Code

## ROC curves

```{r}
# Caret functions
twoClassSummary(predictions, lev = levels(predictions$obs))

prSummary(predictions, lev = levels(predictions$obs))
```

```{r}
# Non-caret functions
result_roc = roc(response = predictions$obs, predictor = predictions$Control, auc=TRUE)

plot(result_roc, print.thres="best", print.thres.best.method="closest.topleft")

result_coords = coords(result_roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
```





