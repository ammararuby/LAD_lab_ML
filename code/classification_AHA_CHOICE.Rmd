---
title: "R Notebook"
output: html_notebook
---

# Libraries

```{r, include = FALSE}
library(tidyverse)
library(microbiome)
library(phyloseq)
library(caret)
library(randomForest)
library(MButils)
```

# Load data

I am using the AHA dataset for writing my code because it is the simplest dataset I can work with.
Simplest = significant number of baseline samples without issue of repeated measures, balanced distribution of classes, likely difference to be detected present.


```{r}
a_ps = readRDS("/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/AHA/220714_MN00462_0226_A000H3WKM5/20221110_results/ps_objects/AHA_trnl_with_metadata_20221205.rds")

a_ps
```

```{r}
c_ps = readRDS("/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/CHOICE/CHOICE_Trnl/CHOICE_20220912/ps_objects/choice_complete_20221205_clean.rds")

c_ps
```

# Preprocess

## Remove problematic samples

Taking out samples that are not listed to have completed the study and samples with more than 0 reads reduces samples from 95 down to 77.

Taking only the baseline timepoint brings further down to 44 samples

Keeping non-completed samples reduced the AUC of modelling substantially, remove these samples as before.

```{r}
a_ps = a_ps %>%
  subset_samples(completion == "Completed" & reads >0) %>%
  subset_samples(timepoint == "Baseline")

a_ps

```

Class distribution in this dataset:
Case = 20
Control = 24

```{r}
sample_data(a_ps) %>%
  as.data.frame() %>%
  as_tibble() %>%
  group_by(group) %>%
  summarise(count = n())
```
## Remove unassigned taxa

Debating whether this should be done for machine learning. Because we are simply throwing out because we don't know what there are and even if they account for a small percentage of reads, they might still have important information.

I think I will refrain from removing unless I have a good reason to.

Don't do this because some very important taxa for ML model turned out to be unassigned. - 12/8/22

Removing completely unassigned taxa reduces taxa from 235 to 82.

```{r}
# a_ps = a_ps %>%
#   subset_taxa(is.na(superkingdom) == FALSE)
# 
# a_ps
```

## Merge phyloseq

```{r}
combined = merge_phyloseq(c_ps, a_ps)

combined
```

### Aligning seq table

```{r}
seqtab = combined@otu_table

seqtab_merged = dada2::collapseNoMismatch(seqtab)

combined = phyloseq(otu_table(seqtab_merged, taxa_are_rows=FALSE),
               sample_data(combined@sam_data), 
               tax_table(combined@tax_table))

combined

#sample_data(combined)
```

## Data transform

```{r}
otu_clr = abundances(combined, "clr") %>% 
  t()

combined = phyloseq(otu_table(otu_clr, taxa_are_rows = FALSE),
                   sample_data(combined@sam_data),
                   tax_table(combined@tax_table))

rm(otu_clr)
```

## Feature table

### Extract data

Order of asv colnames in OTU table and tax table is the same.
```{r}
features = combined@otu_table %>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample")

feature_labels = c("sample")

features
```
### Zero variance features

No features have zero variance
```{r}
nzv = features %>%
  select(!sample) %>%
  nearZeroVar(saveMetrics = TRUE)
```

```{r}
nzv %>%
  filter(nzv == TRUE)
```


```{r}
rm(nzv)
```

### Correlated predictors

```{r}
feature_cor = features %>%
 select(!sample) %>%
 cor() 

summary(feature_cor[upper.tri(feature_cor)])
```

```{r}
high_cor = findCorrelation(feature_cor, cutoff = .50) #using a more permissive cutoff because a lot of columns still left in the input table

filtered_features = features[,-high_cor]
```

```{r}
rm(high_cor)
rm(feature_cor)
```

### Linear dependencies

```{r}
feature_ld = filtered_features %>%
  select(!sample) %>%
  findLinearCombos()

filtered_features = filtered_features[, - feature_ld$remove]
```

## Input table
```{r}
sample_labels = combined@sam_data%>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample") %>%
  select(sample, group) %>%
  left_join(filtered_features)

input = sample_labels %>%
  filter(str_detect(sample, "AHA")) %>%
  mutate(group_shuffle = sample(group))

external_test = sample_labels %>%
  filter(!str_detect(sample, "AHA")) %>%
  select(-c(group))
```

# Modeling

## Baseline performance

```{r}
label_list = c("group", "group_shuffle")
results = loop_label(iterations = 20, input, label_list, percent = 0.8)

auc_df = results[[2]]

importance_df = results[[1]]
```

## Loop

```{r}
label = "group"

importance_df = data.frame()

cummulative_external = data.frame()

auc_list = c()

current_data = input %>%
      select(-label_list[label_list != label]) %>%
      select(-sample)

for(i in 1:20) {
  # Data splitting
    train_ind = caret::createDataPartition((current_data %>%
                                              pull(label)), #split based on label column
                                     p = 0.8,
                                     list = FALSE,
                                     times = 1)
    train = current_data[train_ind,]
    test = current_data[-train_ind,]
    
    # Training model
    
    formula = label %>%
      paste0("~.") %>%
      as.formula()
    
    fit_control = caret::trainControl(method = "CV", # 5-fold cross validation
                                 number = 5,
                                 summaryFunction = prSummary,
                                 classProbs = T)
    
    model = caret::train(formula, #using formula specified 
              data = train,
                  method = "rf",
                  trControl = fit_control,
                  verbose = FALSE, 
                  metric = "AUC")
    
    # Model predictions
    
    predictions = predict(model, newdata = test, type= "prob")
    
    predictions$obs = test %>%
      pull(label) %>%
      as.factor() 
    
    # Evaluate model
    
    eval = MLeval::evalm(predictions, showplots = FALSE, silent = TRUE)
    
    auc = eval$stdres$`Group1`[13,1]
    
    auc_list = append(auc_list, auc)
    
    #Extract feature importance

    feature_importance = caret::varImp(model$finalModel) %>% # Ben calls this importances instead of final model but the rankings are exactly the same, only number value scale is different.
      arrange(desc(Overall)) %>%
      as.matrix() %>%
      t() %>%
      as.data.frame()
    
    feature_importance$auc = auc
    
    feature_importance$iteration = i
    
    feature_importance$label = label
    
    importance_df = rbind(importance_df, feature_importance)
    
    #Predicting on external dataset
    
    external_predictions =  predict(model, newdata = external_test, type= "prob")
    
    external_predictions$sample = external_test$sample
    
    external_predictions$auc = auc
    
    external_predictions$pred = predict(model, newdata = external_test)%>%
      as.factor ()
    
    external_predictions$iteration = i
    
    cummulative_external = rbind(cummulative_external, external_predictions)
    
}

auc_list

importance_df

cummulative_external  
  
```

# Plotting

## AUC plot and t-test
```{r}
data.frame(auc = auc_list)%>%
  ggplot()+
  geom_boxplot(aes(y = auc))+
  theme_classic()
```

## Save external predictions

### predictions
```{r}
max = max(auc_list)

samdf = c_ps@sam_data%>%
  as.data.frame() %>%
  as_tibble(rownames = NA) %>%
  rownames_to_column("sample") %>%
  select(sample, id, treatment, true_week, timepoint) 

save = cummulative_external %>%
  #filter(auc == max) %>%
  left_join(samdf) %>%
  arrange(iteration, treatment, id, true_week)
```

```{r}
write_csv(save, "/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/Machine_learning/LAD_lab_ML/code/aha_to_choice_predictions.csv")
```


### variable importances

```{r}
write_csv(importance_df, "/Users/aa370/Library/CloudStorage/Box-Box/project_davidlab/LAD_LAB_Personnel/Ammara_A/Projects/Machine_learning/LAD_lab_ML/code/aha_to_choice_feature_importance.csv")
```






